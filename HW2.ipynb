{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aF3bkrZNSA9"
      },
      "source": [
        "#**Deep Learning Homework 2: *Optimize and Train Deep Models***\n",
        "### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n",
        "### 2nd semester - 6 ECTS\n",
        "### Prof. Alessandro Sperduti, Prof. NicolÃ² Navarin and Dr. Luca Pasa\n",
        "---\n",
        "In this homework, we will explore how to develop a simple Deep Neural Network for a classification problem. We will learn how to use one of the most popular libraries for Deep Learning, namely [PyTorch ðŸ”¥](https://pytorch.org/). Then, we will see how to face a well known problem that is common during the training phase---overfitting on the training set. Finally, we will study how to perform a fair model selection.\n",
        "\n",
        "<u>Disclaimer</u>: PyTorch is a complex framework actively used by researcher and developers all over the word. It is highly customizable and there are many other packages built along it. The purpose of this and the next homeworks is not to go into all the details of its functionalities, but rather to give you an overview of its applications and understand its rationale. There are many good tutorials online and we suggest you to look at the official [Learn the Basics Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html), in particular if you are not familiar with other Deep Learning Frameworks.\n",
        "\n",
        "We hope that you will be able to use PyTorch by yourself for your future projects!\n",
        "\n",
        "---\n",
        "##**Important Instructions for Submissions:**\n",
        "\n",
        "Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqHD-0bAm6y7"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G81vdttiN4Gr"
      },
      "source": [
        "In this first exercise we will develop a deep feed forward neural network to perform text classification.\n",
        "\n",
        "Let's start by importing the libraries we will need (this may take some minutes) and setting a couple of environmental variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xsTBJxuhhCy1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.0\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EBZ4iQYnJpuT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: skorch in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.0)\n",
            "Requirement already satisfied: torchtext in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.17.2)\n",
            "Requirement already satisfied: torchview in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.6)\n",
            "Requirement already satisfied: hiddenlayer in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3)\n",
            "Requirement already satisfied: portalocker in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.23.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.1.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.9.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (4.66.1)\n",
            "Requirement already satisfied: requests in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext) (2.28.1)\n",
            "Requirement already satisfied: torch==2.2.2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext) (2.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (1.11.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.2.2->torchtext) (2024.3.1)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\simen\\appdata\\roaming\\python\\python311\\site-packages (from portalocker) (305)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (3.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\simen\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.14.0->skorch) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchtext) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.2.2->torchtext) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\simen\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.2.2->torchtext) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install skorch torchtext torchview hiddenlayer portalocker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4MTTs-m3qG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j1YNozG9CRwL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import CoLA\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.helper import SliceDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "import multiprocessing\n",
        "from timeit import default_timer as timer\n",
        "from torchview import draw_graph\n",
        "import matplotlib.pyplot as plt\n",
        "import hiddenlayer as hl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BNgvj4naZajF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.__version__='2.2.2+cpu'\n",
            "device=device(type='cpu')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvidia-smi' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "logging.disable(logging.WARNING)\n",
        "\n",
        "print(f\"{torch.__version__=}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{device=}\")\n",
        "!nvidia-smi --format=csv --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "rng = np.random.default_rng(seed=4242)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt1fhAbVmpTx"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqQZIK0WO908"
      },
      "source": [
        "### Load Dataset: CoLA\n",
        "\n",
        "In this HW, we use the Corpus of Linguistic Acceptability (CoLA) dataset that is available in Torchtext. The CoLA dataset in its full form consists of $10657$ sentences from $23$ linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version provided contains $9594$ sentences belonging to training and development (a.k.a. validation) sets, and excludes $1063$ sentences belonging to a test set. We are thus dealing with text binary classification, wheter a sentence is acceptable or not (0=unacceptable, 1=acceptable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b1nwREGYCfSb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "Package `torchdata` not found. Please install following instructions at https://github.com/pytorch/data",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get data splits\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train, val, test \u001b[38;5;241m=\u001b[39m \u001b[43mCoLA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[0;32m      5\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;66;03m# the first item of each sample (x[0]) is not useful for our case and hence discarded\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Simen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[1;34m(root, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(new_root):\n\u001b[0;32m    192\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(new_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Simen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[1;34m(root, split, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m _check_default_set(split, splits, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[1;32m--> 155\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_datasets(\u001b[38;5;28mtuple\u001b[39m(result), split)\n",
            "File \u001b[1;32mc:\\Users\\Simen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\datasets\\cola.py:74\u001b[0m, in \u001b[0;36mCoLA\u001b[1;34m(root, split)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"CoLA dataset\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m.. warning::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m:rtype: (str, int, str)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileOpener, GDriveReader, HttpReader, IterableWrapper  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     79\u001b[0m url_dp \u001b[38;5;241m=\u001b[39m IterableWrapper([URL])\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: Package `torchdata` not found. Please install following instructions at https://github.com/pytorch/data"
          ]
        }
      ],
      "source": [
        "# Get data splits\n",
        "train, val, test = CoLA(root=\"dataset\", split=('train', 'dev', 'test'))\n",
        "\n",
        "# Preprocess\n",
        "preprocess = lambda x: (x[1], x[2]) # the first item of each sample (x[0]) is not useful for our case and hence discarded\n",
        "train_data = list(map(preprocess, train))\n",
        "val_data = list(map(preprocess, val))\n",
        "test_data = list(map(preprocess, test))\n",
        "\n",
        "rng.shuffle(train_data)\n",
        "rng.shuffle(val_data)\n",
        "rng.shuffle(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoVtzid4WZcw"
      },
      "outputs": [],
      "source": [
        "# Check sizes\n",
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJruLYheWpLe"
      },
      "outputs": [],
      "source": [
        "# This dataset is not balanced, making it harder to predict the least frequent class\n",
        "balance_check = lambda x: sum([x[k][0] for k in range(len(x))]) / len(x)\n",
        "print(f\"{100*balance_check(train_data):.2f}% of training data are labeled 1\")\n",
        "print(f\"{100*balance_check(val_data):.2f}% of validation data are labeled 1\")\n",
        "print(f\"{100*balance_check(test_data):.2f}% of test data are labeled 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwRCr_jybQE"
      },
      "outputs": [],
      "source": [
        "# We will try to reduce this imbalance by oversampling 0-labeled data points. There are other possible approaches to deal with imbalanced datasets, e.g. tweaking the loss function.\n",
        "neg_train_data = [train_data[k] for k in range(len(train_data)) if train_data[k][0] == 0]\n",
        "train_data += neg_train_data\n",
        "neg_val_data = [val_data[k] for k in range(len(val_data)) if val_data[k][0] == 0]\n",
        "val_data += neg_val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaNC38I6OziD"
      },
      "outputs": [],
      "source": [
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gGtyYkDXtXG"
      },
      "outputs": [],
      "source": [
        "print(f\"{100*balance_check(train_data):.2f}% of training data are labeled 1\")\n",
        "print(f\"{100*balance_check(val_data):.2f}% of validation data are labeled 1\")\n",
        "print(f\"{100*balance_check(test_data):.2f}% of test data are labeled 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS1B3TjjtoAe"
      },
      "source": [
        "And have a look at one sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdoLOjK2CfVR"
      },
      "outputs": [],
      "source": [
        "idx = 10\n",
        "sample_label, sample_text = train_data[idx]\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Label: {sample_label}\") # 0=gramatically correct; 1=gramatically wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpTSr7pruC0u"
      },
      "source": [
        "### Data Preprocessing\n",
        "We will go though the basic data processing building blocks for raw text string.\n",
        "Given a string, the first step is to tokenize the data to prepare it as input for the model. For this purpouse, we will use the basic Tokenizer of PyTorch. As you can see, it just normalizes the text, removes whitespaces and unsupported characters, and outputs a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktz69_M1DITk"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "print(\"Example: \", tokenizer(\"I'm so HAPPY to study Deep Learning! #UNIPD #UniLIFE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpIcUP8AuKMg"
      },
      "source": [
        "The CoLA dataset consists of simple sequences of words. Since we have to encode each sample in a single tensor with a fixed number of elements, we will build a vocabulary of the sentences available in our training set. In other words, we tokenize all the training set, and map each unique word into a number (a.k.a. index). This is necesseray as Neural Networks process numbers, so such vocabulary simply translates between readable words and numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw0KabxGEGqj"
      },
      "outputs": [],
      "source": [
        "def create_tokens(dataset):\n",
        "  for sample in dataset:\n",
        "    yield tokenizer(sample[1])\n",
        "\n",
        "# We limit our model to learn from the first 100 most frequent tokens\n",
        "vocab = build_vocab_from_iterator(create_tokens(train_data), specials=[\"<unk>\"], max_tokens=100) # <unk> is the index we use for specials characters ...\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) # ... and 0 as the default values for tokens out of the vocabulary (OOV)\n",
        "print(f\"Our vocabulary is made of {len(vocab)} tokens-index pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6FeI-HavMKA"
      },
      "source": [
        "This vocabulary was build from the training set. Let us see what it does on our example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ZoAGVPCfc0"
      },
      "outputs": [],
      "source": [
        "# Let us define some useful functions to handle tokens and labels\n",
        "text_pipeline = lambda x: vocab(tokenizer(x)) # useful function to go from string -> tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApAbzk_3Cffc"
      },
      "outputs": [],
      "source": [
        "sample_tokenization = text_pipeline(sample_text)\n",
        "print(f\"Samlpe text:\\n {sample_text}\")\n",
        "print(f\"Sample text to tokens:\\n {sample_tokenization}\")\n",
        "print(f\"Label:\\n {sample_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g030w_uaiKoM"
      },
      "source": [
        "So each word is correctly mapped into a number, where most frequent words come first (see how the article \"the\", which is very frequent, gets translated to the number $2$). Notice also how \"fool\" takes the default index value of $0$, meaning that is not frequent in the training set.\n",
        "\n",
        "Neural Netowrks (usually) take input of fixed size, but these sentences have different lenghts! To tackle this issue, one way is to resort to _multi-hot-encoding_ our dataset, means turning it into a tensor of $0$ and $1$. Concretely, this would mean for instance turning the sequence `[3, 5]` into a $100$-dimensional vector (our vocabulary) that would be all zeros except for indices 3 and 5, which would be ones. The obtained input representation indicates which words are present (at least one time) in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQgZmtZiFE7X"
      },
      "outputs": [],
      "source": [
        "def multi_freq_hot(token_list, n_cat=len(vocab)):\n",
        "  encoded = [token_list.count(i) if i in token_list else 0 for i in range(n_cat)]\n",
        "  return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqWBKCchxby4"
      },
      "source": [
        "The next step is building a PyTorch  `Dataset` and `DataLoader`. Without going too much into the details, a `DataLoader` is the object one uses in the `for` loops when iterating over the data for training or inference. It has some additional benefits and pourpouses over iterating usual lists, such as multiprocessing, data transformation, sampling or shuffling. Don't worry too much for now, as we do this part for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJCAhS76FFDL"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    super().__init__()\n",
        "    self.dataset = dataset\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  def __getitem__(self, idx):\n",
        "    # This makes sure we preprocess the data as above ready for PyTorch format\n",
        "    label, text = self.dataset[idx]\n",
        "    txt = multi_freq_hot(text_pipeline(text))\n",
        "    lb, txt = torch.tensor(label, dtype=torch.float32, device=device), torch.tensor(txt, dtype=torch.float32, device=device)\n",
        "    return lb.view(1), txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF5JMnjfGuCx"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(train_data)\n",
        "val_dataset = MyDataset(val_data)\n",
        "test_dataset = MyDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHUTf_g8Gy75"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataloader_training = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_validation = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2-mHR7wbOM5"
      },
      "outputs": [],
      "source": [
        "# Let's see what we get at every iteration of training DataLoaders: a batch of 32 labels (size = 1) and texts (size = 100)\n",
        "next(iter(dataloader_training))[0].shape, next(iter(dataloader_training))[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcn4Z74SmwaC"
      },
      "source": [
        "# Exercise 2.1: Model Definition and Training\n",
        "We can now define the 3-layers feed forward linear model!\n",
        "In PyTorch we do that by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method.\n",
        "\n",
        "In detail, we created a `FeedforwardNetwork` class. The constructor takes three arguments: `input_dim` (an integer), which is the dimension of the input layer; `num_classes` (an integer), which is the number of output classes; and `hidden_layers_dim` (a list of integers), which is the dimension of the hidden layers in the network.\n",
        "\n",
        "In the constructor, the `nn.ModuleList() ` object is initialized to hold the layers of the network. If `hidden_layers_dim` is an empty list, then the network only consists of a single linear layer (from input to output). Otherwise, the network consists of multiple linear layers. The first layer goes from the input layer to the first hidden layer, and the subsequent hidden layers go from the previous hidden layer to the next hidden layer. The final output layer goes from the last hidden layer to the output layer.\n",
        "\n",
        "The `_init_weights` function is a helper function that initializes the weights of the linear layers in the network (you can try it out yourself).\n",
        "\n",
        "The `forward` function defines the forward pass of the network. It first checks whether the network has only one layer (in which case it simply returns the output of that layer). Otherwise, it applies the `relu` activation function to the output of each hidden layer and passes the result to the next layer. Finally, it returns the output of the final layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdtoVP6B_YmA"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, num_classes, hidden_layers_dim=[]):\n",
        "    super().__init__()\n",
        "    if num_classes == 2: num_classes = num_classes -1\n",
        "    self.layers = nn.ModuleList()\n",
        "    if len(hidden_layers_dim) == 0:\n",
        "      self.layers = self.layers.append(nn.Linear(input_dim, num_classes))\n",
        "    else:\n",
        "      for layer_idx in range(len(hidden_layers_dim)):\n",
        "        if layer_idx == 0:  # first layer, from input to hidden\n",
        "          self.layers = self.layers.append(nn.Linear(input_dim, hidden_layers_dim[layer_idx]))\n",
        "        else:  # hidden layers, depending on the input\n",
        "          self.layers = self.layers.append(nn.Linear(hidden_layers_dim[layer_idx-1], hidden_layers_dim[layer_idx]))\n",
        "      self.layers = self.layers.append(nn.Linear(hidden_layers_dim[-1], num_classes))  # final output layer\n",
        "    # self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        # module.weight.data.normal_(mean=0.0, std=.1)\n",
        "        torch.nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x):\n",
        "    if len(self.layers) == 1:\n",
        "      return torch.sigmoid(self.layers[0](x))\n",
        "    else:\n",
        "      for layer in self.layers[:-1]:\n",
        "        x = F.relu(layer(x))\n",
        "    return torch.sigmoid(self.layers[-1](x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x77rElGcHJrn"
      },
      "outputs": [],
      "source": [
        "# Now we prepare our model for this specific dataset and exercise\n",
        "num_class = len(set([label for (label, text) in train_data]))\n",
        "vocab_size = len(vocab)\n",
        "EPOCHS = 40\n",
        "lr = 1e-5\n",
        "model = FeedForwardNetwork(vocab_size, num_class, hidden_layers_dim=[64, 32]).to(device)\n",
        "# Let us see a nice drawing of our Netwokrk :-)\n",
        "model_graph = draw_graph(model, input_size=(batch_size, vocab_size), device=device)\n",
        "model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lexuSj2IFo3"
      },
      "outputs": [],
      "source": [
        "# We need to defin our loss function (Binary Cross Entropy for binary classification) and optimizer\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uzGOnHS0uHu"
      },
      "source": [
        "Finally, we can define the training procedure. In PyTorch there are few rules to follow to train a model, and they became pretty standard and widespreaded. This time try on your own to understand what each line of code is doing and refer to the official documentation, it will be required for the following HWs and exercises!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMMREK-5I2xP"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, dataloader_train, dataloader_val, EPOCHS=EPOCHS):\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  history1 = hl.History() # This is a simple tool for logging\n",
        "  canvas1 = hl.Canvas() # This is a simple tool for plotting\n",
        "  for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (label, text) in enumerate(dataloader_train):\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(text)\n",
        "      loss = criterion(logits, label)\n",
        "      total_loss_train += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "      accuracy = (pred_label == label).sum().item()\n",
        "\n",
        "      total_acc_train += accuracy\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "\n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (label, text) in enumerate(dataloader_val):\n",
        "            logits = model(text)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "            total_acc_val += (pred_label == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val)\n",
        "    if epoch % 1 == 0:\n",
        "      history1.log(epoch, train_loss=avg_loss_train, train_accuracy=accuracy_train,\n",
        "                   val_loss=avg_loss_val, val_accuracy=accuracy_val)#,\n",
        "\n",
        "      with canvas1:\n",
        "        canvas1.draw_plot([history1[\"train_loss\"], history1[\"val_loss\"]])\n",
        "        canvas1.draw_plot([history1[\"train_accuracy\"], history1[\"val_accuracy\"]])\n",
        "\n",
        "  return loss_train, acc_train, loss_val, acc_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWOoJxe3cp-"
      },
      "source": [
        "Let's train it! ðŸ‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa54R_1MKbWb"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDSRMkpISTQ"
      },
      "source": [
        "## Q1: Custom Activation Function [TO COMPLETE]\n",
        "So far we have provided you a basic working model. As first exercise, we ask you to extend it by implementing a custom activation function. This means that you have to:\n",
        "1. Define a class `CustomFeedForwardNetwork` similar to the previous example, but which additionaly takes an `activation_function` at initialization.\n",
        "2. Implement the `activation_function` correctly in the class and forward pass.\n",
        "3. Implement from scratch any activation function. Do not use directly the ones available in `torch.nn.functional`, but you can get some inspiration there.\n",
        "4. Train a model with your custom activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uwe5h3WKan1"
      },
      "source": [
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNJxBiE3KdRF"
      },
      "outputs": [],
      "source": [
        "class CustomFeedForwardNetwork(nn.Module):\n",
        "  def __init__(# TO COMPLETE):\n",
        "    super().__init__()\n",
        "    pass # TO COMPLETE\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass # TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1narBcYLSI1"
      },
      "outputs": [],
      "source": [
        "def activation_function(# TO COMPLETE):\n",
        "  pass # TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b97lODBCLI7Q"
      },
      "outputs": [],
      "source": [
        "num_class = len(set([label for (label, text) in train_data]))\n",
        "vocab_size = len(vocab)\n",
        "EPOCHS = 40\n",
        "lr = 1e-5\n",
        "custom_model = CustomFeedForwardNetwork(# TO COMPLETE).to(device)\n",
        "model_graph = draw_graph(custom_model, input_size=(batch_size, vocab_size), device=device)\n",
        "model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUlpaDq4LP--"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(custom_model, optimizer, dataloader_training, dataloader_validation)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq1vex7WKbfQ"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2rNayCQ3tzl"
      },
      "source": [
        "## Q2: Evaluate the Model [TO COMPLETE]\n",
        "\n",
        "Now, let's go back on our first model. It has been optimized on the training set, and as you can see the performance on the validation set in quite similar (so it does not overfit the training data). Let's now evaluate the performance of our model using the test set. By having a look at the previous code, compute the performances on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73rQ8jooC1I7"
      },
      "source": [
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deXxro4Mf7R6"
      },
      "outputs": [],
      "source": [
        "model.eval() # This is important!\n",
        "total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n",
        "for idx, (label, text) in enumerate(dataloader_test):\n",
        "    pass # TO COMPLETE\n",
        "accuracy_test = total_acc_test/total_count_test\n",
        "loss_test = loss/n_batches_test\n",
        "print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n",
        "print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpNJQzQXC2HE"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQaJFqRVNon"
      },
      "source": [
        "# Exercise 2.2: Overfitting\n",
        "\n",
        "A common problem that occurs when you train a deep neural network is overfittig. Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data. In other words, the model learned patterns specific to the training data, which are irrelevant in other cases.\n",
        "As we have seen in the previous exercise, our model does not much overfit the training data. In this exercise, we try to modify the training parameters in order to have a model that overfits.\n",
        "Overfitting can have many causes and usually is a combination of some of them, for instance: too many parameters/layers, too few training samples, wrong learning rate (usualy too high), ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_UTGw7AENfn"
      },
      "source": [
        "## Q3: Overfitting Model [TO COMPLETE]\n",
        "In the next cell define a new model (from the class `FeedForwardNetwork`) that overfits the training data; then plot the trend of the accuracy and loss in training and validation set.\n",
        "\n",
        "As a reference, you should get that at the end the training accuracy is even $>30\\%$ than the validation accuracy, that on the contrary sould stay quite constant thourghout the epochs. The more your model overfits, the easier it will be to carry out the following exercises.\n",
        "\n",
        "\n",
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfheR-DcjQgc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 40\n",
        "lr = 1e-4\n",
        "overfit_model = FeedForwardNetwork(# TO COMPLETE).to(device)\n",
        "optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n",
        "model_graph = draw_graph(overfit_model, input_size=(batch_size, vocab_size), device=device)\n",
        "model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAUvFjfAcmYf"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(overfit_model, optimizer, dataloader_training, dataloader_validation)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhWJoj7QEYdY"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpIi4-OBVWmq"
      },
      "source": [
        "# Exercise 2.3: $L^p$-regularization\n",
        "One possible way to solve the overitting issue is by using regularization methods. The two most common regularization methods in Deep Learning are the L1-norm regularization and the L2-norm regularization. Both These techniques are based on limiting the capacity of models, by adding a parameter norm penalty to the loss function $\\mathcal{L}$:\n",
        "$$\n",
        "\\hat{\\mathcal{L}}(\\theta,\\mathbf{X},\\mathbf{y}) = \\mathcal{L}(\\theta,\\mathbf{X},\\mathbf{y}) + \\lambda L^p(\\theta)\n",
        "$$\n",
        "where $\\lambda$ is a hyperparameter that weighs the relative contribution of the norm penalty $L^p$ on a lernable parameter vector $\\theta$:\n",
        "$$\n",
        " L^p(\\theta)=||\\theta||_p=\\left(\\sum_i |\\theta_i|^p\\right)^{1/p}\n",
        "$$\n",
        "\n",
        "**[TO COMLPETE]**: Implement the $L^2$ regularization for the overfitting model. Such regularization can either be implemented from scratch or using `torch.linalg` methods, but not using the implementations embedded in Torch optimizers!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8c2h8JEGtHD"
      },
      "source": [
        "### Q4: $L^2$ Regularization [TO COMPLETE]\n",
        "\n",
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pCqS8PjQymc"
      },
      "outputs": [],
      "source": [
        "def train_L2(model, optimizer, dataloader_train, dataloader_val, l2_lambda, EPOCHS=EPOCHS): # <--- We pass the lambda hyperparameter\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  history1 = hl.History()\n",
        "  canvas1 = hl.Canvas()\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (label, text) in enumerate(dataloader_train):\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(text)\n",
        "      loss = criterion(logits, label)\n",
        "      ##########################################################\n",
        "      l2_norm =  # TO COMPLETE\n",
        "      loss_w_l2 = # TO COMPLETE\n",
        "      ##########################################################\n",
        "      total_loss_train += loss\n",
        "      loss_w_l2.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "      accuracy = (pred_label == label).sum().item()\n",
        "\n",
        "      total_acc_train += accuracy\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "\n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (label, text) in enumerate(dataloader_val):\n",
        "            logits = model(text)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "            total_acc_val += (pred_label == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val)\n",
        "    if epoch % 1 == 0:\n",
        "      history1.log(epoch, train_loss=avg_loss_train, train_accuracy=accuracy_train,\n",
        "                   val_loss=avg_loss_val, val_accuracy=accuracy_val)\n",
        "      with canvas1:\n",
        "        canvas1.draw_plot([history1[\"train_loss\"], history1[\"val_loss\"]])\n",
        "        canvas1.draw_plot([history1[\"train_accuracy\"], history1[\"val_accuracy\"]])\n",
        "\n",
        "  return loss_train, acc_train, loss_val, acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLcj3cf5Sl1N"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 40\n",
        "lr = 1e-4\n",
        "# Load the previous (untrained) overfit_model with the same parameters\n",
        "overfit_model = FeedForwardNetwork(# TO COMPLETE).to(device)\n",
        "optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n",
        "model_graph = draw_graph(overfit_model, input_size=(batch_size, vocab_size), device=device)\n",
        "model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWA5rlIUQRsU"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "l2_lambda = # TO COMPLETE\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train_L2(overfit_model, optimizer, dataloader_training, dataloader_validation, l2_lambda=l2_lambda)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xwD7uY1GxV1"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLTJUqX7VhuV"
      },
      "source": [
        "# Exercise 2.4: Early Stopping\n",
        "Early Stopping is a form of regularization used to avoid overfitting. It is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. In order to evaluate the generalization error, early stopping requires that a validation dataset is evaluated during training. Then, when the validation error does not improve for a specific number of epochs (a.k.a. the \"patience\" or \"tolerance\" hyperparameter), it stops the training phase.\n",
        "\n",
        "To implement it in PyTorch, we define a simple classes that does this job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVPRIb1gQyoo"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance, min_delta):\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, train_loss, validation_loss):\n",
        "        if (validation_loss - train_loss) > self.min_delta:\n",
        "            self.counter +=1\n",
        "            if self.counter >= self.tolerance:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeLHARUEHWsE"
      },
      "source": [
        "### Q5: Early Stopping [TO COMPLETE]\n",
        "\n",
        "Define a training procedure with Early Stopping class and check its performances.\n",
        "\n",
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF2WyxJzWVZZ"
      },
      "outputs": [],
      "source": [
        "def train_early_stop(model, optimizer, dataloader_train, dataloader_val, tolerance, min_delta, EPOCHS=EPOCHS): # <--- We pass the hyperparameters\n",
        "  ###########################################################\n",
        "  early_stopping # TO COMPLETE\n",
        "  ###########################################################\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  history1 = hl.History()\n",
        "  canvas1 = hl.Canvas()\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (label, text) in enumerate(dataloader_train):\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(text)\n",
        "      loss = criterion(logits, label)\n",
        "      total_loss_train += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "      accuracy = (pred_label == label).sum().item()\n",
        "\n",
        "      total_acc_train += accuracy\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "\n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (label, text) in enumerate(dataloader_val):\n",
        "            logits = model(text)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            pred_label = torch.where(logits > 0.5, 1.0, 0.0)\n",
        "            total_acc_val += (pred_label == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val)\n",
        "    if epoch % 1 == 0:\n",
        "\n",
        "      history1.log(epoch, train_loss=avg_loss_train, train_accuracy=accuracy_train,\n",
        "                   val_loss=avg_loss_val, val_accuracy=accuracy_val)\n",
        "\n",
        "      with canvas1:\n",
        "        canvas1.draw_plot([history1[\"train_loss\"], history1[\"val_loss\"]])\n",
        "        canvas1.draw_plot([history1[\"train_accuracy\"], history1[\"val_accuracy\"]])\n",
        "\n",
        "    ############################################################\n",
        "    # TO COMPLETE - Implement Early Stopping\n",
        "    ############################################################\n",
        "  return loss_train, acc_train, loss_val, acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUgQIJvBXcLP"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 40\n",
        "lr = 1e-4\n",
        "# Load the previous (untrained) overfit_model with the same parameters\n",
        "overfit_model = FeedForwardNetwork(# TO COMPLETE).to(device) # CANCEL THIS\n",
        "optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n",
        "model_graph = draw_graph(overfit_model, input_size=(batch_size, vocab_size), device=device)\n",
        "model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gybj6eSEXowL"
      },
      "outputs": [],
      "source": [
        "# Define Early Stopping parameters\n",
        "tolerance = # TO COMPLETE\n",
        "min_delta = # TO COMPLETE\n",
        "\n",
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train_early_stop(overfit_model, optimizer, dataloader_training, dataloader_validation, tolerance, min_delta)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auc9q8FWHmT6"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpE49B6UVj1C"
      },
      "source": [
        "# Exercise 2.5: Model Selection\n",
        "Hyperparameters are the parameters of the learning method itself which we have to specify a priori, i.e., before model fitting. In contrast, model parameters are parameters which arise as a result of the fit (the network weights). The aim of model selection is selecting the best hyperparameters for our deep network. Finding the right hyperparameters for a model can be crucial for the model performance on given data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itlx4eMGVwhF"
      },
      "source": [
        "### Q6: Grid Search [TO COMPLETE]\n",
        "\n",
        "Since a deep net has many hyperparameters, to find the best ones we have to consider different possible combinations of values for each hyperparameter. One common method to perform this complex task is Grid-Search.\n",
        "Given a set of values for each hyperparameter, the Grid-Search algorithm will build a model on all possible parameter combinations. It iterates through every parameter combination and stores a model for each combination. Finally, the model that obtained the best result on the validation set will be selected and tested.\n",
        "\n",
        "In order to perfrom Grid-Search we will use the `GridSearchCV` class from `scikit-learn`, together with some classes from the `skorch` package which allows to use PyTorch models with `scikit-learn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moDGnfYBVy0Y"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetClassifier(\n",
        "    module=FeedForwardNetwork,\n",
        "    criterion=torch.nn.CrossEntropyLoss,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    max_epochs=20)\n",
        "\n",
        "# we need to adapt PyTorch Dataset to work with Scikit-Learn GridSearchCV\n",
        "# we use the SliceDataset class from skorch package for this purpose\n",
        "X_slice = SliceDataset(train_dataset, idx=1)\n",
        "y_slice = SliceDataset(train_dataset, idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXOA474unH3P"
      },
      "source": [
        "Let's define the lists of hyper-parameters' values. Also in this case, we use dictionaries of very limited size and hyperparameters, but in a real-world scenario a reasonable amount of possible values should be considered (and there are better search algorthims besisdes Grid-Search).\n",
        "\n",
        "**[TO COMPLETE]**: Define a Grid Search of plausible hyperparameters:\n",
        "\n",
        "\n",
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcSIrBz2eZLI"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'lr': [], # TO COMPLETE\n",
        "    'module__input_dim': [], # TO COMPLETE\n",
        "    'module__num_classes': [], # TO COMPLETE\n",
        "    'module__hidden_layers_dim': [], # TO COMPLETE\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppvMDaeI2PX"
      },
      "source": [
        "`%ENDCODE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOrBbzWVnX6Z"
      },
      "source": [
        "Now you can use `sklearn`'s `GridSearchCV` to search the hyperparameter space of the `NeuralNetClassifier`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsK_gOiPekcG"
      },
      "outputs": [],
      "source": [
        "n_jobs = multiprocessing.cpu_count()-1\n",
        "print(f\"Parallel jobs: {n_jobs}\")\n",
        "gs = GridSearchCV(model, params, n_jobs=n_jobs, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIhXvWonjZav"
      },
      "outputs": [],
      "source": [
        "outputs = gs.fit(X_slice, y_slice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr-SpSE7nyrl"
      },
      "source": [
        "Print the best hyper-parameters and the performances on the best model on the test set:\n",
        "\n",
        "`%STARTCODE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF9x4VoniCYm"
      },
      "outputs": [],
      "source": [
        "print(\"Best score: {:.3f}, Best Params: {}\".format(gs.best_score_, gs.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICBmyrHwJAix"
      },
      "source": [
        "`%STARTCODE`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
