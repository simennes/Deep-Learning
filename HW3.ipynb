{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Deep Learning Homework 3: Convolutional Neural Networks**\n",
        "### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n",
        "### 2nd semester - 6 ECTS\n",
        "### Prof. Alessandro Sperduti, Prof. Nicol√≤ Navarin and Dr. Luca Pasa\n",
        "---\n",
        "In this homework, we will explore how to develop a simple Convolutional Neural Network for image classification. In the first part, we will learn how to develop a simple CNN, while in the second part we will explore the impact of various hyper-parameters on the learning performances."
      ],
      "metadata": {
        "id": "uqSn7VUKVmwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "Let's start importing the libraries we will need and setting a couple of environmental variables."
      ],
      "metadata": {
        "id": "5FEro6dyR9ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install skorch torchview torchinfo hiddenlayer portalocker"
      ],
      "metadata": {
        "id": "3jdp8UnEVoL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d, MaxPool2d, Linear\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import SVHN\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Lambda\n",
        "import numpy as np\n",
        "from torchview import draw_graph\n",
        "from torchinfo import summary\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import hiddenlayer as hl\n",
        "from timeit import default_timer as timer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #Conflict of nn.functional.act_f and torch.act_f\n",
        "\n",
        "print(\"You are using:\") # We tested with Python 3.10.12 and torch.__version__='2.2.1+cu121'\n",
        "!python --version\n",
        "print(f\"{torch.__version__=}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"{device=}\")\n",
        "!nvidia-smi --format=csv --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(43)\n",
        "rng = np.random.default_rng(seed=4242)"
      ],
      "metadata": {
        "id": "ltsxWUQTVt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.1: Simple CNN"
      ],
      "metadata": {
        "id": "PWawaA5OyrEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Data Loading and Preprocessing [TO COMPLETE]\n",
        "\n",
        "We will use the  The Street View House Numbers `SVHN` dataset. This dataset comes from the real-world problem of recognizing digits and numbers in natural scene images (SVHN is obtained from house numbers in Google Street View). It consists of $10$ classes, 1 for each digit. Orignially, there are $73257$ digits for training and $26032$ digits for testing, but we will use a subset of those to speed up computations. Each sample is a $32\\times32$ pixels color image (thus with an extra $\\times3$ dimensions for the colors channels), associated with a label from one of the classes:\n",
        "\n",
        "```python\n",
        "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "```\n",
        "\n",
        "We will divide the dataset in training, testing and validation set. As you already know, the training set will be used to train the model, the validation set will be used to perform model selection and finally, the test set will be used to asses the performance of the network."
      ],
      "metadata": {
        "id": "2Ko0IxcEJdDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[TO COMPLETE]**\n",
        "\n",
        "In the standard dataset, each pixel intensity is represented by a `uint8` (byte) from $0$ to $255$. As a preprocessing step, we will rescale these values in the range $[0,1]$. You should write a simple so-called MinMaxScaler which takes as input a PIL Image (a specific format for images in Python) and rescales it, after making the appropriate type and shape transformations."
      ],
      "metadata": {
        "id": "49Xj_gqoShHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "M3RegtIXS8wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MinMaxScaler(img):#[TO COMPLETE]\n",
        "  # First, we load the data as numpy array\n",
        "  img_as_array = np.asarray(img)\n",
        "  # TO COMPLETE: Transform the array to Tensor for PyTorch\n",
        "\n",
        "  # TO COMPLETE: image is of size (Height, Width, Channels). But torch excepts (C, H, W). Hence, the second thing is to permute.\n",
        "\n",
        "  # TO COMPLETE: Rescale images form [0,255] to [0,1]\n",
        "  normalized_img = ...\n",
        "  return normalized_img\n",
        "\n",
        "\n",
        "'''\n",
        "NEW DATA LOADING\n",
        "Note that the previous function must be passed to transforms.Lambda().\n",
        "'''\n",
        "dataset = SVHN(root='./data/', download=True, split=\"train\", transform=transforms.Lambda(MinMaxScaler))\n",
        "testing_data = SVHN(root='./data/', download=True, split=\"test\", transform=transforms.Lambda(MinMaxScaler))"
      ],
      "metadata": {
        "id": "ez87LDF9gkTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "7GV3m_ZqTNrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain in the cell below why it is usually a good idea to ensure that all the features have a similar scale."
      ],
      "metadata": {
        "id": "HP_A3NzEyHcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "VhXlGGmCyR2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **[TO COMPLETE]**"
      ],
      "metadata": {
        "id": "qFLwpkgmySLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "l64Y2StLySWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the classes and dataset shapes:\n",
        "classes = sorted(np.unique(dataset.labels))\n",
        "print(f\"{classes=}\")\n",
        "print(f\"Training Dataset shape: {dataset.data.shape}\")\n",
        "print(f\"Test set shape: {testing_data.data.shape}\")"
      ],
      "metadata": {
        "id": "5Xc5SaQoxyfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also check whether the dataset is balanced, i.e. there are the same amount of samples for each class\n",
        "label_count = {}\n",
        "for _, idx in dataset:\n",
        "    label = classes[idx]\n",
        "    if label not in label_count:\n",
        "        label_count[label] = 0\n",
        "    label_count[label] += 1\n",
        "label_count"
      ],
      "metadata": {
        "id": "nJ_45u1oALt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is skewed towards classes 1 and 2, and one should adopt extra care in this cases. However, this is not the focus of the HW. Let us split the dataset into training/validation/test sets:"
      ],
      "metadata": {
        "id": "J3SRYMaDehzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 20000 # We consider subsets of the original dataset to speed up computations\n",
        "val_size = 2000\n",
        "test_size = 2000\n",
        "\n",
        "# Get Validation set from the Train dataset\n",
        "rest_size = len(dataset) - (train_size + val_size)\n",
        "train_data, val_data, _ = random_split(dataset, [train_size, val_size, rest_size])\n",
        "\n",
        "# Sample form Test set\n",
        "rest_size = len(testing_data) - test_size\n",
        "test_data, _ = random_split(testing_data, [test_size, rest_size])\n",
        "\n",
        "print(f\"Training samples = {len(train_data)} \\nValidation samples = {len(val_data)} \\nTest samples = {len(test_data)}\")"
      ],
      "metadata": {
        "id": "iQPH8tH_iwkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how a normalized sample looks like..."
      ],
      "metadata": {
        "id": "TKrBXv2PfFYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 50 # 51st data sample\n",
        "print(train_data[idx]) # (image, label) tensors\n",
        "assert train_data[idx][0].max().item() <= 1., \"Check your MinMaxScaler!\""
      ],
      "metadata": {
        "id": "K3sLGEsRkjYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... but it's better to see it as images!"
      ],
      "metadata": {
        "id": "Fm1tImt7fQ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_chan, img_height, img_width = train_data[idx][0].shape #Channel, Height, Width\n",
        "\n",
        "# Plots\n",
        "n_rows = 5\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols*1.4, n_rows * 1.6))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        image, label = train_data[index]\n",
        "        image = image.permute((1, 2, 0)) #C,H,W -> H,W,C\n",
        "        plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(classes[label])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N86f2-vTk859"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since `train_data`, `val_data` and `test_data` are already PyTorch Datasets, we can use directly DataLoaders to load data from them."
      ],
      "metadata": {
        "id": "YZwAHcuf2Frd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "dataloader_training = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_data, batch_size=batch_size)\n",
        "dataloader_test = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "n7xoYIa8GvaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Definition and Training\n",
        "\n",
        "Let's create a simple CNN. The model will be composed of:\n",
        "* One 2D Convolutional layer with kernel size $3\\times3$ and $32$ output filters/features, that use ReLU activation function;\n",
        "* a Max Pooling layer (2D) of size $2\\times2$;\n",
        "* a Flatten layer;\n",
        "* a final Dense layer with 10 output neurons (one per class). We do not need to normalize or transform further the outputs, as the `CrossEntropyLoss` takes care of that. Another equivalent approach would be to add a `LogSoftmax` final layer that returns log-probabilities and then use the `NegativeLogLikelihoodLoss`., and with the `softmax` activation function to ensure that the sum of all the estimated class probabilities for each image is equal to 1.\n",
        "Note that as `input_shape` attribute's value in the first layer report also the third dimension that represents the channel."
      ],
      "metadata": {
        "id": "Lfm2p1PUJnhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Convolutional_Network(nn.Module):\n",
        "  def __init__(self, conv_filters=[], kernel_sizes=[], max_pool_sizes=[], act_fs=[], verbose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert len(conv_filters) == len(kernel_sizes), \"length of {conv_filters} and {kernel_sizes} must be same\"\n",
        "    assert len(conv_filters) == len(max_pool_sizes), \"length of {conv_filters} and {max_pool_sizes} must be same\"\n",
        "    # max pool of [1, 1] corresponds to no max pool\n",
        "    assert len(conv_filters) == len(act_fs), \"length of {conv_filters} and {act_fs} must be same\"\n",
        "\n",
        "    self.conv_layers = nn.ModuleList()\n",
        "    self.max_pools = nn.ModuleList()\n",
        "    self.in_chan = img_chan\n",
        "    self.in_height = img_height\n",
        "    self.in_width = img_width\n",
        "    self.output_dim = len(classes)#10\n",
        "    self.verbose = verbose\n",
        "    self.act_fs = act_fs\n",
        "\n",
        "    height_dimension = self.in_height\n",
        "    width_dimension = self.in_width\n",
        "\n",
        "    for maxp1, maxp2 in max_pool_sizes: # as long as padding='same'\n",
        "      height_dimension = height_dimension // maxp1\n",
        "      width_dimension = width_dimension // maxp2\n",
        "\n",
        "    self.inp_dim_to_linear = conv_filters[-1] * height_dimension * width_dimension\n",
        "\n",
        "    for idx in range(len(conv_filters)):\n",
        "      if idx == 0:\n",
        "        # Conv2d(in_channels, out_channels, kernel_size, padding)\n",
        "        self.conv_layers = self.conv_layers.append(Conv2d(self.in_chan, conv_filters[idx],\n",
        "                                                          kernel_sizes[idx], padding='same'))\n",
        "      else:\n",
        "        self.conv_layers = self.conv_layers.append(Conv2d(conv_filters[idx-1], conv_filters[idx],\n",
        "                                                          kernel_sizes[idx], padding='same'))\n",
        "\n",
        "      self.max_pools = self.max_pools.append(MaxPool2d(max_pool_sizes[idx]))\n",
        "\n",
        "    self.linear = Linear(self.inp_dim_to_linear, self.output_dim)\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print(f\"My model has {pytorch_total_params} trainable parameters.\")\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    for idx in range(len(self.conv_layers)):\n",
        "      from_shape = x.shape[1:]\n",
        "      active_conv_layer = self.conv_layers[idx]\n",
        "      active_max_pool = self.max_pools[idx]\n",
        "      active_act_fun = self.act_fs[idx]\n",
        "      x = active_max_pool(active_act_fun(active_conv_layer(x)))\n",
        "      to_shape = x.shape[1:]\n",
        "\n",
        "      if self.verbose:\n",
        "        print(f'From dimension [{from_shape}] to dimension [{to_shape}]')\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1) # if start_dim=1 missed, it also consider batch_size\n",
        "\n",
        "    if self.verbose:\n",
        "        print(f'From dimension [{to_shape}] to dimension [{x.shape[1:]}]')\n",
        "        print(f'From dimension [{x.shape[1:]}] to dimension [{self.output_dim}]')\n",
        "\n",
        "    return self.linear(x) # no need to use softmax because of the loss function"
      ],
      "metadata": {
        "id": "-X2zBp_BG5iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_filters = [32]\n",
        "kernel_sizes = [[3, 3]]\n",
        "max_pool_sizes = [[2, 2]]\n",
        "act_fs = [F.relu]\n",
        "\n",
        "num_epochs = 20\n",
        "lr = 1e-3\n",
        "model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes,\n",
        "                                 act_fs, False).to(device)\n",
        "\n",
        "# Let's visualize the model\n",
        "model_graph = draw_graph(model, input_size=(batch_size, 3, 32, 32), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "DsKtE5pOfd84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is another tool that gives a nice summary of our model!\n",
        "summary(model, input_size=(batch_size, 3, 32, 32))"
      ],
      "metadata": {
        "id": "jrbRkVmyFi6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Parameters of a CNN [TO COMPLETE]"
      ],
      "metadata": {
        "id": "SwY5uAFZTlEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain in the cell below how the number of parameters for the Conv2D layers are determined. Perform the calculations to calculate the parameters of the Conv2D layer."
      ],
      "metadata": {
        "id": "5tiNlSeZtezn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "D3iMaFMwswsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: **[TO COMPLETE]**\n",
        "\n"
      ],
      "metadata": {
        "id": "qIMuYZX2suKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "GimDOIlNszFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model."
      ],
      "metadata": {
        "id": "awAy6UY-1jnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "A4pPhIofg8_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, dataloader_train, dataloader_val, epochs, hparam_tuning=False):\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  history1 = hl.History() # This is a simple tool for logging\n",
        "  canvas1 = hl.Canvas() # This is a simple tool for plotting\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (img, label) in enumerate(dataloader_train):\n",
        "      img, label = img.to(device), label.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(img)\n",
        "      loss = criterion(logits, label)\n",
        "      total_loss_train += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_acc_train += (logits.argmax(1) == label).sum().item()\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "\n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (img, label) in enumerate(dataloader_val):\n",
        "            img, label = img.to(device), label.to(device)\n",
        "            logits = model(img)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            total_acc_val += (logits.argmax(1) == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val)\n",
        "    if epoch % 1 == 0:\n",
        "      if not hparam_tuning:\n",
        "        history1.log(epoch, train_loss=avg_loss_train, train_accuracy=accuracy_train,\n",
        "                     val_loss=avg_loss_val, val_accuracy=accuracy_val)\n",
        "\n",
        "        with canvas1:\n",
        "          canvas1.draw_plot([history1[\"train_loss\"], history1[\"val_loss\"]])\n",
        "          canvas1.draw_plot([history1[\"train_accuracy\"], history1[\"val_accuracy\"]])\n",
        "      else:\n",
        "        print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n",
        "        print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n",
        "  return loss_train, acc_train, loss_val, acc_val\n",
        "\n",
        "def plot_learning_acc_and_loss(loss_tr, acc_tr, loss_val, acc_val):\n",
        "    # Helper function to plot again accuracy and loss\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.grid()\n",
        "    plt.plot(range(len(acc_tr)), acc_tr, label='acc_training')\n",
        "    plt.plot(range(len(acc_tr)), acc_val, label='acc_validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.grid()\n",
        "    plt.plot(range(len(acc_tr)), loss_tr, label='loss_training')\n",
        "    plt.plot(range(len(acc_tr)), loss_val, label='loss_validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-GnNWzZrjVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs = num_epochs)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ],
      "metadata": {
        "id": "Uy-bG-y2qJRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we test the model:"
      ],
      "metadata": {
        "id": "nnu_aFjRAZRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader_test=dataloader_test):\n",
        "  model.eval()\n",
        "  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n",
        "  for idx, (img, label) in enumerate(dataloader_test):\n",
        "      img, label = img.to(device), label.to(device)\n",
        "      logits = model(img)\n",
        "      loss += criterion(logits, label)\n",
        "      total_acc_test += (logits.argmax(1) == label).sum().item()\n",
        "      total_count_test += label.size(0)\n",
        "      n_batches_test += 1\n",
        "  accuracy_test = total_acc_test/total_count_test\n",
        "  loss_test = loss/n_batches_test\n",
        "  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n",
        "  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"
      ],
      "metadata": {
        "id": "5x7_x8X1nF3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model)"
      ],
      "metadata": {
        "id": "sQ4rNwrMq-6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the test accuracy of our simple CNN should be about $77\\%$."
      ],
      "metadata": {
        "id": "F3a4YlGm1RQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Filters\n",
        "Let's now visualize the filters learned in the first convolutional layer."
      ],
      "metadata": {
        "id": "u9431xgoThpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights\n",
        "layer_names = []\n",
        "weights = {}\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        layer_names.append(name)\n",
        "        weights.setdefault(name, param.data)\n",
        "\n",
        "conv_weights = weights['conv_layers.0.weight']\n",
        "print(f\"{conv_weights.shape=} <=> [out_dim, inp_dim, kernel_size[0], kernel_size[1]]\")"
      ],
      "metadata": {
        "id": "lNLIXlsVPcPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_w0 = conv_weights[:, 0, :, :] # Pick one input dimension\n",
        "\n",
        "# Rescale weights for visualization\n",
        "conv_w0 -= torch.min(conv_w0)\n",
        "conv_w0 /= torch.max(conv_w0)\n",
        "\n",
        "for r in range(4):\n",
        "    for c in range(8):\n",
        "        n=r*8+c\n",
        "        plt.subplot(4, 8, n+1)\n",
        "        plt.imshow(conv_w0[n,:,:].cpu(), interpolation='none')\n",
        "        plt.title(f'chan{n+1}')\n",
        "        plt.axis('off')\n",
        "        plt.gray()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AKtAqvuwT3Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They might be a bit hard to interpret, but it seems that the various filters have learned to detect different shapes and positions."
      ],
      "metadata": {
        "id": "HgWPQbYbuGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep CNN\n",
        "\n",
        "\n",
        "Let's consider a deeper model, more precisly in this exercise we consider a model composed of:\n",
        "* One 2D convolutional layer with kernel size $3\\times3$ and $32$ output filters/features, that use ReLu activation function;\n",
        "* a Max Pooling layer (2D) of size $2\\times2$;\n",
        "* One 2D convolutional layer with kernel size $2\\times2$ and $16$ output filters/features, that use ReLu activation function;\n",
        "* a Max Pooling layer (2D) of size $2\\times2$;\n",
        "* a Flatten layer;\n",
        "* a final Dense layer with $10$ output neurons (one per class), and with the `softmax` activation function.\n"
      ],
      "metadata": {
        "id": "dZtcrs38XM3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_filters = [32, 16]\n",
        "kernel_sizes = [[3, 3], [2, 2]]\n",
        "max_pool_sizes = [[2, 2], [2, 2]]\n",
        "act_fs = [F.relu, F.relu]\n",
        "\n",
        "num_epochs = 20\n",
        "lr = 1e-3\n",
        "deep_model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes,\n",
        "                                      act_fs, False).to(device)\n",
        "\n",
        "model_graph = draw_graph(deep_model, input_size=(batch_size, 3, 32, 32), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "Ye2w8dCuXSQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(deep_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "NKSNZijCVJSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(deep_model, optimizer, dataloader_training, dataloader_validation, epochs = num_epochs)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ],
      "metadata": {
        "id": "iOPNAMWHYuw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(deep_model)"
      ],
      "metadata": {
        "id": "BbAcxuQkoBT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What?! By developing a deeper CNN (although it has fewer parameters), we just slighted increased the accuracy...it seems like we need to develop a better CNN."
      ],
      "metadata": {
        "id": "jskjJB_93Xm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.2: A Better CNN\n",
        "\n",
        "Let's develop a network that performs better than the very simple one above. This exercise aims to explore how much the various hyper-parameters influence the classification capability of the model.\n",
        "\n",
        "**Q3 [TO COMPLETE]**: Your task is to modify some of the hyper-parameters of the previous exercise's network and compare the results. The requirements are as follows:\n",
        "1. what your deemed to be your **best models** must show an improvement in the test set results (generalization) over the result of the baseline model used in the previous exercise;\n",
        "2. in the code cell between `%STARTCODE` and `%ENDCODE` report the code of **only** your **best model**;\n",
        "3. make sure that in the code cell between `%STARTCODE` and `%ENDCODE` you report also its result on the test set and plot the accuracy and the loss trends;\n",
        "4. for each setup you tested, analyze and discuss the obtained results briefly in the last *text* cell at the bottom, motivating the choice of picking your **best model**.\n",
        "\n",
        "Hint: Each reparameterization should change a different aspect in the network, while the rest of the parameters would stay the same. You can then jointly tune different hyperparamentes.\n",
        "Example of parameters to tune are (we suggest to test at least one re-parametrization for each of these categories):\n",
        "\n",
        "*   number of layers or neurons or filters dimension;\n",
        "*   activation functions;\n",
        "*   epochs;\n",
        "*   max-pooling on/off on certain layers, or pool size;\n",
        "*   learning rate;\n",
        "*   ...\n",
        "\n"
      ],
      "metadata": {
        "id": "tYiMa1cxaB2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "Qa6mcdofREf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to edit the following (e.g. with for-loops or however you prefer)\n",
        "n_epochs = # TO COMPLETE\n",
        "conv_filters = # TO COMPLETE\n",
        "kernel_sizes = # TO COMPLETE\n",
        "max_pool_sizes = # TO COMPLETE\n",
        "act_fs = # TO COMPLETE\n",
        "lr = # TO COMPLETE\n",
        "\n",
        "model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes, act_fs, False).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "start = timer()\n",
        "# Notice hparam_tuning=True\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, EPOCHS, hparam_tuning=True)\n",
        "end = timer()\n",
        "\n",
        "print(f\"Training time in second: {(end - start)}\")\n",
        "plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)\n",
        "test(model)\n",
        "print(\"-\"*85)"
      ],
      "metadata": {
        "id": "YKk437rNRDsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Discuss your results [TO COMPLETE]\n",
        "\n",
        "In the discussion, you need to clearly motivate your choice of hyperparameters, what work and what did not work, and how you picked your best model. Most of the HW grade depends on this, so to support you we provide a answer guideline."
      ],
      "metadata": {
        "id": "RQcbxs6NRU9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "Of_nb4jqRXCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model that I found is... **[TO COMPLETE]**\n",
        "\n",
        "The achieved accuracy in the test set is... **[TO COMPLETE]**\n",
        "\n",
        "Discussion:\n",
        "**[TO COMPLETE]**"
      ],
      "metadata": {
        "id": "RzlXX86y6VaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides, I tested also other models:\n",
        "* **[TO COMPLETE]**\n",
        "* ..\n",
        "\n",
        "\n",
        "Discussion:\n",
        "**[TO COMPLETE]**"
      ],
      "metadata": {
        "id": "j_aCzHZh6bCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "si3hFOSIRZvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4: Data Augmentation [TO COMPLETE]\n",
        "\n",
        "Besides hyperparameter tuning, another approach to improve performances is _Data Augmentation_, i.e. we exploit our available data to get new samples with some heuristrics and tricks, with the aim to improve generalization. In practice, you can have a look [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html) for some examples, and it has been shown that a valid technique for SVHN is _inverting_ the colors of the RGB images (since the label is the number at the center of the image, croping, flipping and rotating might change the data completely in a way that the label does not anymore correspond to the number at the center of the image).\n",
        "\n",
        "**[TO COMPLETE]**\n",
        "Reload and augment the SVHN dataset, and along your `MinMaxScaler`, use the `invert` function from `torchvision.transforms.functional` to invert the pixels of a random $1/3$ of the (smaller) training/validation set.\n",
        "Retrain your **best model** on this new dataset and check whether it improves generalization performances."
      ],
      "metadata": {
        "id": "Hdc6poI3OeqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "a_gzfDbTXs3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################### DATA AUGMENTATION ###############################\n",
        "from torchvision.transforms.functional import invert\n",
        "\n",
        "# In torch, transformations have to be composed\n",
        "# Concatenate MinMaxScaler, and a Transformation that inverts randomly 1/3 of the samples\n",
        "# ...\n",
        "data_augmentation = transforms.Compose(Lambda(MinMaxScaler),\n",
        "                                       # TO COMPLETE)\n",
        "\n",
        "'''\n",
        "NEW DATA LOADING\n",
        "Note that the previous function must be passed to transforms.Lambda().\n",
        "'''\n",
        "new_dataset = SVHN(root='data/', download=True, split=\"train\", transform=data_augmentation) # 50000 samples\n",
        "\n",
        "train_size = 30000\n",
        "val_size = 2000\n",
        "rest_size = len(new_dataset) - (train_size + val_size)\n",
        "new_train_data, new_val_data, _ = random_split(new_dataset, [train_size, val_size, rest_size])\n",
        "\n",
        "# We do not augment the test data!\n",
        "testing_data = SVHN(root='data/', download=True, split=\"test\", transform=Lambda(MinMaxScaler)) # 10000 samples\n",
        "test_size = 2000\n",
        "rest_size = len(testing_data) - test_size\n",
        "new_test_data, _ = random_split(testing_data, [test_size, rest_size])\n",
        "\n",
        "\n",
        "print(f\"Training samples = {len(new_train_data)} \\nValidation samples = {len(new_val_data)} \\nTest samples = {len(new_test_data)}\")"
      ],
      "metadata": {
        "id": "lpC-AHMvLpmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "aug_dataloader_training = DataLoader(new_train_data, batch_size=batch_size, shuffle=True)\n",
        "aug_dataloader_validation = DataLoader(new_val_data, batch_size=batch_size)\n",
        "aug_dataloader_test = DataLoader(new_test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "JpoTVCKZJO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your best model\n",
        "conv_filters = # TO COMPLETE\n",
        "kernel_sizes = # TO COMPLETE\n",
        "max_pool_sizes = # TO COMPLETE\n",
        "act_fs = # TO COMPLETE\n",
        "num_epochs = # TO COMPLETE\n",
        "lr = # TO COMPLETE\n",
        "\n",
        "aug_model = My_Convolutional_Network(conv_filters, kernel_sizes, max_pool_sizes,\n",
        "                                 act_fs, False).to(device)\n",
        "\n",
        "model_graph = draw_graph(aug_model, input_size=(batch_size, 3, 32, 32), device=device)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "a7i91_Q47pfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(aug_model.parameters(), lr=lr)\n",
        "\n",
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(aug_model, optimizer, aug_dataloader_training, aug_dataloader_validation, num_epochs, False)\n",
        "end = timer()"
      ],
      "metadata": {
        "id": "uRVwlKww78cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results of my best CNN with Data Augmentation:\")\n",
        "test(aug_model, aug_dataloader_test)"
      ],
      "metadata": {
        "id": "ALEGfMFG8uk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "tUijjhBRREaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with ResNet18 + ImageNet"
      ],
      "metadata": {
        "id": "7nl9nbhsxgyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now experiment with transfer learning. We will now load the model structure and weights of a small [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) (which is still pretty big!), pretrained on the ImageNet dataset. We will then add a fully connected layer at the end of the network and fine-tune it on the SVHM dataset. In this way, we can leverage the knowledge already present in the pre-trained weight and transfer it on our task!"
      ],
      "metadata": {
        "id": "ixsJSGmwGjGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights"
      ],
      "metadata": {
        "id": "rYi1ipiUFqMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESNET18 = resnet18(pretrained=True)\n",
        "#model_graph = draw_graph(RESNET18, input_size=(1, 3, 32, 32), device=device, expand_nested=True)\n",
        "#model_graph.visual_graph\n",
        "summary(RESNET18, input_size=(batch_size, 3, 32, 32))"
      ],
      "metadata": {
        "id": "dj4x39INycYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ResNet has a final fully connected layer that generates 1000 logits corresponding to the classes in the ImageNet dataset. We will re-define it and make it map the features learned in the previous layer to the 10 classes of the SVHM datset."
      ],
      "metadata": {
        "id": "XpWkgXIeHnsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer_input = RESNET18.fc.in_features\n",
        "linear_layer_output = RESNET18.fc.out_features\n",
        "print(f\"Last layer: {linear_layer_input} -> {linear_layer_output}\")"
      ],
      "metadata": {
        "id": "xWpT1c3G2vU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace  the last layer\n",
        "RESNET18.fc = Linear(linear_layer_input, 10)"
      ],
      "metadata": {
        "id": "bZMjSbVL34JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer_input = RESNET18.fc.in_features\n",
        "linear_layer_output = RESNET18.fc.out_features\n",
        "print(f\"Last layer: {linear_layer_input} -> {linear_layer_output}\")"
      ],
      "metadata": {
        "id": "yDswGss14CMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we \"freeze\" the other weights in the network so that only the ones in the last layer are trainable."
      ],
      "metadata": {
        "id": "ZU8itp1eH8WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in RESNET18.named_parameters():\n",
        "    if name == 'fc.weight' or name == 'fc.bias':\n",
        "      param.requires_grad = True\n",
        "    else:\n",
        "      param.requires_grad = False"
      ],
      "metadata": {
        "id": "7anuWdD62b5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESNET18 = RESNET18.to(device) # Let's now load it to GPU"
      ],
      "metadata": {
        "id": "3Yzd6yG_yREw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a smaller subset in order to speed-up the fine-tuning process:"
      ],
      "metadata": {
        "id": "-eksr6l6LOP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_idxs = list(range(0, len(train_data), 5))  # ~ 4000 Tr\n",
        "val_idxs = list(range(1, len(val_data), 5))  # ~ 400 Val\n",
        "test_idxs = list(range(1, len(test_data), 5))  # ~ 400 Ts\n",
        "\n",
        "train_data_sub = torch.utils.data.Subset(train_data, train_idxs)\n",
        "val_data_sub = torch.utils.data.Subset(val_data, val_idxs)\n",
        "test_data_sub = torch.utils.data.Subset(test_data, test_idxs)"
      ],
      "metadata": {
        "id": "Yfv8a8j48-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_idxs), len(val_idxs), len(test_idxs)"
      ],
      "metadata": {
        "id": "iYlQ6Odofx0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to apply custom preprocessing steps that are expected by the pretrained model, we will define a small wrapper around our dataset."
      ],
      "metadata": {
        "id": "7H5JC628Ndz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = ResNet18_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "class RESDatasetWrapper(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    self.dataset = dataset\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  def __getitem__(self, idx):\n",
        "    image, label = self.dataset[idx]\n",
        "    image = preprocess(image)\n",
        "    label = torch.tensor(label, dtype=torch.long)\n",
        "    return image.to(device), label.to(device)"
      ],
      "metadata": {
        "id": "G-X74d0-Gf4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RES_train_dataset = RESDatasetWrapper(train_data_sub)\n",
        "RES_val_dataset = RESDatasetWrapper(val_data_sub)\n",
        "RES_test_dataset = RESDatasetWrapper(test_data_sub)\n",
        "\n",
        "batch_size=256\n",
        "\n",
        "RES_dataloader_training = DataLoader(RES_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "RES_dataloader_validation = DataLoader(RES_val_dataset, batch_size=batch_size)\n",
        "RES_dataloader_test = DataLoader(RES_test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "2ktgFiaQGd8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "lr = 1e-3\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RESNET18.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "DmY9GHEKxyd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(RESNET18, optimizer,\n",
        "                                                           RES_dataloader_training,\n",
        "                                                           RES_dataloader_validation,\n",
        "                                                           epochs = num_epochs)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ],
      "metadata": {
        "id": "Sj8vU7Yf8RNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(RESNET18, dataloader_test=RES_dataloader_test)"
      ],
      "metadata": {
        "id": "MhzLq8jUAwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we got this result by leveraging a model freely available on the internet and trained on another dataset. That's the power of transfer learning!"
      ],
      "metadata": {
        "id": "rMucQVvEH4ZZ"
      }
    }
  ]
}